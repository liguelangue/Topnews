{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "class ExtractEvent:\n",
    "    def __init__(self):\n",
    "        self.map_dict = self.load_mapdict()\n",
    "        self.minlen = 2\n",
    "        self.maxlen = 30\n",
    "        self.keywords_num = 20\n",
    "        self.limit_score = 10\n",
    "        self.IP = \"(([NERMQ]*P*[ABDP]*)*([ABDV]{1,})*([NERMQ]*)*([VDAB]$)?([NERMQ]*)*([VDAB]$)?)*\"\n",
    "        self.IP = \"([NER]*([PMBQADP]*[NER]*)*([VPDA]{1,}[NEBRVMQDA]*)*)\"\n",
    "        self.MQ = '[DP]*M{1,}[Q]*([VN]$)?'\n",
    "        self.VNP = 'V*N{1,}'\n",
    "        self.NP = '[NER]{1,}'\n",
    "        self.REN = 'R{2,}'\n",
    "        self.VP = 'P?(V|A$|D$){1,}'\n",
    "        self.PP = 'P?[NERMQ]{1,}'\n",
    "        self.SPO_n = \"n{1,}\"\n",
    "        self.SPO_v = \"v{1,}\"\n",
    "        self.stop_tags = {'u', 'wp', 'o', 'y', 'w', 'f', 'u', 'c', 'uj', 'nd', 't', 'x'}\n",
    "        self.combine_words = {\"首先\", \"然后\", \"之前\", \"之后\", \"其次\", \"接着\"}\n",
    "\n",
    "    \"\"\"构建映射字典\"\"\"\n",
    "    def load_mapdict(self):\n",
    "        tag_dict = {\n",
    "            'B': 'b'.split(),  # 时间词\n",
    "            'A': 'a d'.split(),  # 时间词\n",
    "            'D': \"d\".split(),  # 限定词\n",
    "            'N': \"n j s zg en l r\".split(),  #名词\n",
    "            \"E\": \"nt nz ns an ng\".split(),  #实体词\n",
    "            \"R\": \"nr\".split(),  #人物\n",
    "            'G': \"g\".split(),  #语素\n",
    "            'V': \"vd v va i vg vn g\".split(), #动词\n",
    "            'P': \"p f\".split(),  #介词\n",
    "            \"M\": \"m t\".split(),  #数词\n",
    "            \"Q\": \"q\".split(), #量词\n",
    "            \"v\": \"V\".split(), #动词短语\n",
    "            \"n\": \"N\".split(), #名词介宾短语\n",
    "        }\n",
    "        map_dict = {}\n",
    "        for flag, tags in tag_dict.items():\n",
    "            for tag in tags:\n",
    "                map_dict[tag] = flag\n",
    "        return map_dict\n",
    "\n",
    "    \"\"\"根据定义的标签,对词性进行标签化\"\"\"\n",
    "    def transfer_tags(self, postags):\n",
    "        tags = [self.map_dict.get(tag[:2], 'W') for tag in postags]\n",
    "        return ''.join(tags)\n",
    "\n",
    "    \"\"\"抽取出指定长度的ngram\"\"\"\n",
    "    def extract_ngram(self, pos_seq, regex):\n",
    "        ss = self.transfer_tags(pos_seq)\n",
    "        def gen():\n",
    "            for s in range(len(ss)):\n",
    "                for n in range(self.minlen, 1 + min(self.maxlen, len(ss) - s)):\n",
    "                    e = s + n\n",
    "                    substr = ss[s:e]\n",
    "                    if re.match(regex + \"$\", substr):\n",
    "                        yield (s, e)\n",
    "        return list(gen())\n",
    "\n",
    "    '''抽取ngram'''\n",
    "    def extract_sentgram(self, pos_seq, regex):\n",
    "        ss = self.transfer_tags(pos_seq)\n",
    "        def gen():\n",
    "            for m in re.finditer(regex, ss):\n",
    "                yield (m.start(), m.end())\n",
    "        return list(gen())\n",
    "\n",
    "    \"\"\"指示代词替换，消解处理\"\"\"\n",
    "    def cite_resolution(self, words, postags, persons):\n",
    "        if not persons and 'r' not in set(postags):\n",
    "            return words, postags\n",
    "        elif persons and 'r' in set(postags):\n",
    "            cite_index = postags.index('r')\n",
    "            if words[cite_index] in {\"其\", \"他\", \"她\", \"我\"}:\n",
    "                words[cite_index] = persons[-1]\n",
    "                postags[cite_index] = 'nr'\n",
    "        elif 'r' in set(postags):\n",
    "            cite_index = postags.index('r')\n",
    "            if words[cite_index] in {\"为何\", \"何\", \"如何\"}:\n",
    "                postags[cite_index] = 'w'\n",
    "        return words, postags\n",
    "\n",
    "    \"\"\"抽取量词性短语\"\"\"\n",
    "    def extract_mqs(self, wds, postags):\n",
    "        phrase_tokspans = self.extract_sentgram(postags, self.MQ)\n",
    "        if not phrase_tokspans:\n",
    "            return []\n",
    "        phrases = [''.join(wds[i[0]:i[1]])for i in phrase_tokspans]\n",
    "        return phrases\n",
    "\n",
    "    '''抽取动词性短语'''\n",
    "    def get_ips(self, wds, postags):\n",
    "        ips = []\n",
    "        phrase_tokspans = self.extract_sentgram(postags, self.IP)\n",
    "        if not phrase_tokspans:\n",
    "            return []\n",
    "        phrases = [''.join(wds[i[0]:i[1]])for i in phrase_tokspans]\n",
    "        phrase_postags = [''.join(postags[i[0]:i[1]]) for i in phrase_tokspans]\n",
    "        for phrase, phrase_postag_ in zip(phrases, phrase_postags):\n",
    "            if not phrase:\n",
    "                continue\n",
    "            phrase_postags = ''.join(phrase_postag_).replace('m', '').replace('q','').replace('a', '').replace('t', '')\n",
    "            if phrase_postags.startswith('n') or phrase_postags.startswith('j'):\n",
    "                has_subj = 1\n",
    "            else:\n",
    "                has_subj = 0\n",
    "            ips.append((has_subj, phrase))\n",
    "        return ips\n",
    "\n",
    "    \"\"\"分短句处理\"\"\"\n",
    "    def split_short_sents(self, text):\n",
    "        return [i for i in re.split(r'[,，]', text) if len(i)>2]\n",
    "    \"\"\"分段落\"\"\"\n",
    "    def split_paras(self, text):\n",
    "        return [i for i in re.split(r'[\\n\\r]', text) if len(i) > 4]\n",
    "\n",
    "    \"\"\"分长句处理\"\"\"\n",
    "    def split_long_sents(self, text):\n",
    "        return [i for i in re.split(r'[;。:； ：？?!！【】▲丨|]', text) if len(i) > 4]\n",
    "\n",
    "    \"\"\"移出噪声数据\"\"\"\n",
    "    def remove_punc(self, text):\n",
    "        text = text.replace('\\u3000', '').replace(\"'\", '').replace('“', '').replace('”', '').replace('▲','').replace('” ', \"”\")\n",
    "        tmps = re.findall('[\\(|（][^\\(（\\)）]*[\\)|）]', text)\n",
    "        for tmp in tmps:\n",
    "            text = text.replace(tmp, '')\n",
    "        return text\n",
    "\n",
    "    \"\"\"保持专有名词\"\"\"\n",
    "    def zhuanming(self, text):\n",
    "        books = re.findall('[<《][^《》]*[》>]', text)\n",
    "        return books\n",
    "\n",
    "    \"\"\"对人物类词语进行修正\"\"\"\n",
    "    def modify_nr(self, wds, postags):\n",
    "        phrase_tokspans = self.extract_sentgram(postags, self.REN)\n",
    "        wds_seq = ' '.join(wds)\n",
    "        pos_seq = ' '.join(postags)\n",
    "        if not phrase_tokspans:\n",
    "            return wds, postags\n",
    "        else:\n",
    "            wd_phrases = [' '.join(wds[i[0]:i[1]]) for i in phrase_tokspans]\n",
    "            postag_phrases = [' '.join(postags[i[0]:i[1]]) for i in phrase_tokspans]\n",
    "            for wd_phrase in wd_phrases:\n",
    "                tmp = wd_phrase.replace(' ', '')\n",
    "                wds_seq = wds_seq.replace(wd_phrase, tmp)\n",
    "            for postag_phrase in postag_phrases:\n",
    "                pos_seq = pos_seq.replace(postag_phrase, 'nr')\n",
    "        words = [i for i in wds_seq.split(' ') if i]\n",
    "        postags = [i for i in pos_seq.split(' ') if i]\n",
    "        return words, postags\n",
    "\n",
    "    \"\"\"对人物类词语进行修正\"\"\"\n",
    "    def modify_duplicate(self, wds, postags, regex, tag):\n",
    "        phrase_tokspans = self.extract_sentgram(postags, regex)\n",
    "        wds_seq = ' '.join(wds)\n",
    "        pos_seq = ' '.join(postags)\n",
    "        if not phrase_tokspans:\n",
    "            return wds, postags\n",
    "        else:\n",
    "            wd_phrases = [' '.join(wds[i[0]:i[1]]) for i in phrase_tokspans]\n",
    "            postag_phrases = [' '.join(postags[i[0]:i[1]]) for i in phrase_tokspans]\n",
    "            for wd_phrase in wd_phrases:\n",
    "                tmp = wd_phrase.replace(' ', '')\n",
    "                wds_seq = wds_seq.replace(wd_phrase, tmp)\n",
    "            for postag_phrase in postag_phrases:\n",
    "                pos_seq = pos_seq.replace(postag_phrase, tag)\n",
    "        words = [i for i in wds_seq.split(' ') if i]\n",
    "        postags = [i for i in pos_seq.split(' ') if i]\n",
    "        return words, postags\n",
    "\n",
    "    '''对句子进行分词处理'''\n",
    "    def cut_wds(self, sent):\n",
    "        wds = list(pseg.cut(sent))\n",
    "        postags = [w.flag for w in wds]\n",
    "        words = [w.word for w in wds]\n",
    "        return self.modify_nr(words, postags)\n",
    "\n",
    "    \"\"\"移除噪声词语\"\"\"\n",
    "    def clean_wds(self, words, postags):\n",
    "        wds = []\n",
    "        poss =[]\n",
    "        for wd, postag in zip(words, postags):\n",
    "            if postag[0].lower() in self.stop_tags:\n",
    "                continue\n",
    "            wds.append(wd)\n",
    "            poss.append(postag[:2])\n",
    "        return wds, poss\n",
    "\n",
    "    \"\"\"检测是否成立, 肯定需要包括名词\"\"\"\n",
    "    def check_flag(self, postags):\n",
    "        if not {\"v\", 'a', 'i'}.intersection(postags):\n",
    "            return 0\n",
    "        return 1\n",
    "\n",
    "    \"\"\"识别出人名实体\"\"\"\n",
    "    def detect_person(self, words, postags):\n",
    "        persons = []\n",
    "        for wd, postag in zip(words, postags):\n",
    "            if postag == 'nr':\n",
    "                persons.append(wd)\n",
    "        return persons\n",
    "\n",
    "    \"\"\"识别出名词性短语\"\"\"\n",
    "    def get_nps(self, wds, postags):\n",
    "        phrase_tokspans = self.extract_sentgram(postags, self.NP)\n",
    "        if not phrase_tokspans:\n",
    "            return [],[]\n",
    "        phrases_np = [''.join(wds[i[0]:i[1]]) for i in phrase_tokspans]\n",
    "        return phrase_tokspans, phrases_np\n",
    "\n",
    "    \"\"\"识别出介宾短语\"\"\"\n",
    "    def get_pps(self, wds, postags):\n",
    "        phrase_tokspans = self.extract_sentgram(postags, self.PP)\n",
    "        if not phrase_tokspans:\n",
    "            return [],[]\n",
    "        phrases_pp = [''.join(wds[i[0]:i[1]]) for i in phrase_tokspans]\n",
    "        return phrase_tokspans, phrases_pp\n",
    "\n",
    "    \"\"\"识别出动词短语\"\"\"\n",
    "    def get_vps(self, wds, postags):\n",
    "        phrase_tokspans = self.extract_sentgram(postags, self.VP)\n",
    "        if not phrase_tokspans:\n",
    "            return [],[]\n",
    "        phrases_vp = [''.join(wds[i[0]:i[1]]) for i in phrase_tokspans]\n",
    "        return phrase_tokspans, phrases_vp\n",
    "\n",
    "    \"\"\"抽取名动词性短语\"\"\"\n",
    "    def get_vnps(self, s):\n",
    "        wds, postags = self.cut_wds(s)\n",
    "        if not postags:\n",
    "            return [], []\n",
    "        if not (postags[-1].endswith(\"n\") or postags[-1].endswith(\"l\") or postags[-1].endswith(\"i\")):\n",
    "            return [], []\n",
    "        phrase_tokspans = self.extract_sentgram(postags, self.VNP)\n",
    "        if not phrase_tokspans:\n",
    "            return [], []\n",
    "        phrases_vnp = [''.join(wds[i[0]:i[1]]) for i in phrase_tokspans]\n",
    "        phrase_tokspans2 = self.extract_sentgram(postags, self.NP)\n",
    "        if not phrase_tokspans2:\n",
    "            return [], []\n",
    "        phrases_np = [''.join(wds[i[0]:i[1]]) for i in phrase_tokspans2]\n",
    "        return phrases_vnp, phrases_np\n",
    "\n",
    "    \"\"\"提取短语\"\"\"\n",
    "    def phrase_ip(self, content):\n",
    "        spos = []\n",
    "        events = []\n",
    "        content = self.remove_punc(content)\n",
    "        paras = self.split_paras(content)\n",
    "        for para in paras:\n",
    "            long_sents = self.split_long_sents(para)\n",
    "            for long_sent in long_sents:\n",
    "                persons = []\n",
    "                short_sents = self.split_short_sents(long_sent)\n",
    "                for sent in short_sents:\n",
    "                    words, postags = self.cut_wds(sent)\n",
    "                    person = self.detect_person(words, postags)\n",
    "                    words, postags = self.cite_resolution(words, postags, persons)\n",
    "                    words, postags = self.clean_wds(words, postags)\n",
    "                    #print(words,postags)\n",
    "                    ips = self.get_ips(words, postags)\n",
    "                    persons += person\n",
    "                    for ip in ips:\n",
    "                        events.append(ip[1])\n",
    "                        wds_tmp = []\n",
    "                        postags_tmp = []\n",
    "                        words, postags = self.cut_wds(ip[1])\n",
    "                        verb_tokspans, verbs = self.get_vps(words, postags)\n",
    "                        pp_tokspans, pps = self.get_pps(words, postags)\n",
    "                        tmp_dict = {str(verb[0]) + str(verb[1]): ['V', verbs[idx]] for idx, verb in enumerate(verb_tokspans)}\n",
    "                        pp_dict = {str(pp[0]) + str(pp[1]): ['N', pps[idx]] for idx, pp in enumerate(pp_tokspans)}\n",
    "                        tmp_dict.update(pp_dict)\n",
    "                        sort_keys = sorted([int(i) for i in tmp_dict.keys()])\n",
    "                        for i in sort_keys:\n",
    "                            if i < 10:\n",
    "                                i = '0' + str(i)\n",
    "                            wds_tmp.append(tmp_dict[str(i)][-1])\n",
    "                            postags_tmp.append(tmp_dict[str(i)][0])\n",
    "                        wds_tmp, postags_tmp = self.modify_duplicate(wds_tmp, postags_tmp, self.SPO_v, 'V')\n",
    "                        wds_tmp, postags_tmp = self.modify_duplicate(wds_tmp, postags_tmp, self.SPO_n, 'N')\n",
    "                        if len(postags_tmp) < 2:\n",
    "                            continue\n",
    "                        seg_index = []\n",
    "                        i = 0\n",
    "                        for wd, postag in zip(wds_tmp, postags_tmp):\n",
    "                            if postag == 'V':\n",
    "                                seg_index.append(i)\n",
    "                            i += 1\n",
    "                        spo = []\n",
    "                        for indx, seg_indx in enumerate(seg_index):\n",
    "                            if indx == 0:\n",
    "                                pre_indx = 0\n",
    "                            else:\n",
    "                                pre_indx = seg_index[indx-1]\n",
    "                            if pre_indx < 0:\n",
    "                                pre_indx = 0\n",
    "                            if seg_indx == 0:\n",
    "                                spo.append(('', wds_tmp[seg_indx], ''.join(wds_tmp[seg_indx+1:])))\n",
    "                            elif seg_indx > 0 and indx < 1:\n",
    "                                spo.append((''.join(wds_tmp[:seg_indx]), wds_tmp[seg_indx], ''.join(wds_tmp[seg_indx + 1:])))\n",
    "                            else:\n",
    "                                spo.append((''.join(wds_tmp[pre_indx+1:seg_indx]), wds_tmp[seg_indx], ''.join(wds_tmp[seg_indx + 1:])))\n",
    "                        spos += spo\n",
    "\n",
    "        return events, spos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "import numpy as np\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改content重复执行测试\n",
    "cont=[]\n",
    "f = codecs.open(\"./data/content/content20.txt\", \"r\", \"utf-8\")    # 打开文件以便写入中文\n",
    "line = f.readlines()\n",
    "for li in line:\n",
    "    cont.append(li.strip())\n",
    "    # print(li.strip())\n",
    "f.close()  #  关闭文件\n",
    "\n",
    "# 存储不同类别的三元组结果\n",
    "group=[]\n",
    "handler = ExtractEvent()\n",
    "# start = time.time()\n",
    "for id in range(len(cont)):\n",
    "    events, spos = handler.phrase_ip(cont[id])\n",
    "    spos = [i for i in spos if i[0] and i[2]]\n",
    "    for spo in spos:\n",
    "        # print(spo)\n",
    "        group.append(spo)\n",
    "    \n",
    "fpart = []\n",
    "mpart = []\n",
    "bpart = []\n",
    "for i in range(len(group)):\n",
    "    fpart.append([group[i][0]])\n",
    "    mpart.append([group[i][1]])\n",
    "    bpart.append([group[i][2]])\n",
    "\n",
    "#做映射，相当于词袋\n",
    "dictionary_f = corpora.Dictionary(fpart)#做成字典\n",
    "corpus_f = [dictionary_f.doc2bow(sentence) for sentence in fpart]#在每个句子中都做自然模型\n",
    "#建立LDA模型，传入语料、映射字典、指定要得到多少个主题(无监督)\n",
    "lda_f = gensim.models.ldamodel.LdaModel(corpus=corpus_f, id2word=dictionary_f, num_topics=1) #类似Kmeans自己指定K值\n",
    "\n",
    "#做映射，相当于词袋\n",
    "dictionary_m = corpora.Dictionary(mpart)#做成字典\n",
    "corpus_m = [dictionary_m.doc2bow(sentence) for sentence in mpart]#在每个句子中都做自然模型\n",
    "#建立LDA模型，传入语料、映射字典、指定要得到多少个主题(无监督)\n",
    "lda_m = gensim.models.ldamodel.LdaModel(corpus=corpus_m, id2word=dictionary_m, num_topics=1) #类似Kmeans自己指定K值\n",
    "\n",
    "#做映射，相当于词袋\n",
    "dictionary_b = corpora.Dictionary(bpart)#做成字典\n",
    "corpus_b = [dictionary_f.doc2bow(sentence) for sentence in bpart]#在每个句子中都做自然模型\n",
    "#建立LDA模型，传入语料、映射字典、指定要得到多少个主题(无监督)\n",
    "lda_b = gensim.models.ldamodel.LdaModel(corpus=corpus_b, id2word=dictionary_b, num_topics=1) #类似Kmeans自己指定K值\n",
    "\n",
    "f = codecs.open(\"./data/titles/c20.txt\", \"w\", \"utf-8\")    # 打开文件以便写入中文\n",
    "#分别打印每个主题各个最具代表性的5个关键词\n",
    "for topic in lda_f.print_topics(num_topics=1, num_words=10):\n",
    "    print (topic[1], file = f)\n",
    "\n",
    "for topic in lda_m.print_topics(num_topics=1, num_words=10):\n",
    "    print (topic[1], file = f)\n",
    "\n",
    "for topic in lda_b.print_topics(num_topics=1, num_words=10):\n",
    "    print (topic[1], file = f)\n",
    "f.close()  #  关闭文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = [1082, 496, 344, 109, 118, 128, 0, 145, 71, 52, 60, 120, 124, 457, 338, 248, 84, 0, 0, 175]\n",
    "x = np.array(num)\n",
    "out = np.argsort(-x) # 热度排行\n",
    "# print(out)\n",
    "out = [0, 1, 3, 12, 11, 9, 17, 7, 14, 16, 15, 10, 8, 2,  4, 5, 13, 18, 19, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = []\n",
    "f = codecs.open(\"./data/rank.txt\", \"w\", \"utf-8\")    # 打开文件以便写入中文\n",
    "for i in range(17):\n",
    "    index = 1\n",
    "    for j in out:\n",
    "        if i==j:\n",
    "            line.append(index)\n",
    "            print(\"主题\", index, \"的热度排行是第\", i+1, \"名\",file = f)\n",
    "        index = index+1\n",
    "f.close()  #  关闭文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 14, 3, 15, 16, 20, 8, 13, 6, 12, 5, 4, 17, 9, 11, 10]\n"
     ]
    }
   ],
   "source": [
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f95fa36c7e4c51eacc47621d6b5b12b95b1240208d9d0223f73170116e5fb93b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
